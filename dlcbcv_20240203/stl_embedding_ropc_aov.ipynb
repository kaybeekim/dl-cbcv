{"cells":[{"cell_type":"markdown","metadata":{},"source":["# STL-Embedding ROPC & AOV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"elapsed":3876,"status":"ok","timestamp":1698252479641,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"zSkLguiJA2AI","outputId":"cf9ab9c4-7300-4a99-ef16-350bf81ed5a4"},"outputs":[],"source":["import libs.config as config\n","import libs.hyperparam as hyperparam\n","import libs.util as util \n","import libs.customized_dataset as customized_dataset\n","import libs.models as models \n","import libs.forecasting as forecasting \n","import libs.plots as plots \n","\n","import os  # for interacting with the operating system\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # for debugging cuda errors\n","import glob  # for finding files in directories\n","import warnings \n","warnings.filterwarnings('ignore') # for ignoring all warnings\n","# import argparse  # for parsing command line arguments (for running from the terminal)\n","\n","import random\n","import math # for math operations\n","import time  # for time-related functionalities\n","import holidays # for checking if a date is a holiday\n","import matplotlib.pyplot as plt  # for plotting\n","plt.set_cmap('cividis') # color map for the plots to 'cividis'\n","## Enable the display of matplotlib plots inline in a Jupyter notebook\n","%matplotlib inline \n","import matplotlib.ticker as ticker # for customizing the plots' tick locations and labels\n","import numpy as np  # for numerical computations\n","import pandas as pd  # for data manipulation and analysis\n","pd.set_option('display.max_columns', None) # to display all columns\n","from datetime import date, datetime, timedelta  # for working with dates and times\n","\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler, Normalizer  # for scaling data\n","from tqdm import tqdm  # for creating progress bars\n","\n","import torch  # for building and training neural networks\n","from torch.utils.data import Dataset, DataLoader # for loading and managing datasets\n","import torch.nn as nn  # for building neural networks\n","import torch.nn.functional as F  # for implementing various activation functions\n","import torch.optim as optim  # for defining optimizers\n","\n","# set the device as GPU with index 0\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","config.get_environ_info(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Check NVIDIA GPU"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["### Set data path"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698252479642,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"N8LI4FloRwjj"},"outputs":[],"source":["## Google cluster\n","# READ_DIR = r\"/home/kkim476/dl-cbcv/data/weekly_cohort_data_1000_missing_filled_final\"\n","READ_DIR = r\"/home/kkim476/dl-cbcv/data/selected_ten\"\n","WORK_DIR = r\"/home/kkim476/dl-cbcv\"\n"]},{"cell_type":"markdown","metadata":{},"source":["### Set prediction target, covariates, and save mode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set prediction target\n","PREDICTION_GOAL = 'repeat_order_per_customer' \n","TARGET_TASK = config.get_target_variable_name(PREDICTION_GOAL)\n","\n","### set covariate features\n","USE_EMBEDDING = True\n","COHORT_EMBEDDING = True\n","DUMMY_VAR = False # entity embedding, rather than one hot encoding\n","\n","### folder path for saving results\n","SAVE_MODE = False\n","SAVE_DIR = f'{WORK_DIR}/results/stl_embedding_{PREDICTION_GOAL}_{datetime.today().strftime(\"%Y-%m-%d\")}'\n","SAVE_MODEL, SAVE_EPOCH, SAVE_PLOT, SAVE_PREDICT, SAVE_ACTUAL = config.create_save_folders(SAVE_MODE, SAVE_DIR)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1698252480142,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"4lyMoWePRwdA","outputId":"19e39ca9-e4c8-4947-e7d2-10623afba5ef"},"outputs":[],"source":["## this file is cohort-week level aggregated panel data after ETL of raw earnest transaction DB\n","raw_df, MERCHANT_NAMES_EMB_INT = util.read_files_generate_behaviorfeatures_get_embed_dict(\n","    READ_DIR, hyperparam.TRAIN_START, hyperparam.TEST_START, hyperparam.TEST_END,\n","    group_identifier='acq_week', time_identifier='week',\n","    acquisition_identifier='N_week_cohort',\n","    order_identifier = 'orders',\n","    spend_identifier = 'spend')\n","\n","FREQ, week_start = util.get_week_start(raw_df, hyperparam.TRAIN_START, hyperparam.TEST_START, hyperparam.TEST_END) "]},{"cell_type":"markdown","metadata":{},"source":["### Zero padding\n","\n","This is technical but important piece regarding how to handle 'cohort' triangle data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":1293,"status":"ok","timestamp":1698252481771,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"C9sJ6-lURwVW","outputId":"548f3b42-55ce-4d91-9be9-253beb866318"},"outputs":[],"source":["# columns that I want to keep\n","company_static = ['merchant_index', 'merchant', 'parent_merchant', 'category', 'subcategory']\n","if USE_EMBEDDING:\n","    company_static = company_static + ['merchant_emb_int', 'merchant_name']\n","# company_dynamic = ['cohort_size', 'active_users', \n","#                    'orders','rpt_orders','initial_order',\n","#                    'spend','rpt_spend','initial_spend',\n","#                    'initial_aov','rpt_aov',\n","#                    ]\n","company_dynamic = []\n","\n","assert all(column in list(raw_df.columns) for column in company_static + company_dynamic), \\\n","    'some columns are missing in the raw_df'\n","\n","df_padded = util.zero_padding(raw_df, [TARGET_TASK], company_static, company_dynamic, hyperparam.INPUT_CHUNK_LENGTH, FREQ,\n","                               use_merchant_embedding=True, merchant_name='merchant_name')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate calendar time covariates\n","\n","Our covariates include:\n","- week of year_t+1 (1 week ahead) : (categorical) one-hot encoding. 53 variables.\n","- holidays_t+1 (1 week ahead) : (binary) dummy variable. 1 variable.\n","- global trend_t : (continuous) quadratic. 2 variables.\n","- cohort numbering_i : (continuous) quadratic. 2 variables.\n","- cohort tenure_it : (continuous) quadratic. 2 variables."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698252482117,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"dNrriuNBY7zp","outputId":"8344e916-0226-4c0c-f047-777bcf15d2d7"},"outputs":[],"source":["df_padded_w_cov, country_holidays = util.generate_calendartime_features(df_padded, FREQ, week_start, DUMMY_VAR)\n","# util.check_holidays(df_padded_w_cov, country_holidays)\n","df_padded_w_cov = util.generate_cohort_features(df_padded_w_cov, [TARGET_TASK], COHORT_EMBEDDING, DUMMY_VAR, hyperparam.COHORT_EMB_NUM)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## limit data range\n","df = df_padded_w_cov[df_padded_w_cov['group']<=hyperparam.TEST_END] # limit cohort\n","df = df[df['time']>=hyperparam.TRAIN_START_with_offset][(df['time']<=hyperparam.TEST_END)] # limit time window\n","\n","## get covariate feature names\n","COVARIATE_FEATURE_NAMES, covariate_name_to_index = config.get_covariate_variable_name(df, USE_EMBEDDING, COHORT_EMBEDDING, DUMMY_VAR)\n","print(COVARIATE_FEATURE_NAMES)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del df_padded\n","del df_padded_w_cov\n","del country_holidays"]},{"cell_type":"markdown","metadata":{},"source":["### Split into train:val:test (time-wise) X censored:uncensored (group-wise)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698252483347,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"UUc0EZm2Y7hb"},"outputs":[],"source":["(main_df, main_df_train, main_df_valid, main_df_test, \n"," censored_df, censored_df_train, censored_df_valid, censored_df_test) = util.split_dataframe(\n","    df, hyperparam.TRAIN_START, hyperparam.VAL_START, hyperparam.TEST_START, \n","    hyperparam.VAL_START_with_offset, hyperparam.TEST_START_with_offset, hyperparam.VAL_LOSS)"]},{"cell_type":"markdown","metadata":{},"source":["### Transform data frame to scaled numpy array\n","\n","1. As numpy array is more efficient to handle, we transform pandas data frame into numpy array.\n","\n","2. For each task, we scale the data with its own scaler. This is important for multi-task learning as each task has different scale."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## entire sequences (unscaled)\n","whole_dict = util.df_to_numpy(df, TASKS=[TARGET_TASK], COVARIATES=COVARIATE_FEATURE_NAMES, \n","                              use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","\n","## main cohorts' sequences\n","main_train_dict = util.df_to_scaled_numpy(main_df_train, [TARGET_TASK], COVARIATE_FEATURE_NAMES,\n","                                          use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","main_test_dict = util.df_to_scaled_numpy(main_df_test, [TARGET_TASK], COVARIATE_FEATURE_NAMES,  main_train_dict['scaler'],\n","                                         use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","if hyperparam.VAL_LOSS:\n","    main_val_dict = util.df_to_scaled_numpy(main_df_valid, [TARGET_TASK], COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                            use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","\n","## censored cohorts' sequences\n","censored_train_dict = util.df_to_scaled_numpy(censored_df_train, [TARGET_TASK], COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                              use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","censored_test_dict = util.df_to_scaled_numpy(censored_df_test, [TARGET_TASK], COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                             use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","if hyperparam.VAL_LOSS:\n","    censored_val_dict = util.df_to_scaled_numpy(censored_df_valid, [TARGET_TASK], COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                                use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and DataLoader\n","the creation of custom datasets, and the initialization of the DataLoader.\n","- custom dataset class will group the data by the group column\n","- collate function will handle the padding and attention masking\n","\n","<img src=\"../img/data%20period%20and%20samples.png\" alt=\"data_period\" width=\"600\" height=\"400\">\n","\n","<img src=\"../img/input%20output%20format.png\" alt=\"input_output\" width=\"600\" height=\"400\">\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate train loader and validation loader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1698252486506,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"tv2X8ubGV2YZ"},"outputs":[],"source":["from libs.customized_dataset import CrossSectionalTimeSeriesDataset, collate_fn, value_dict_to_np\n","import multiprocessing\n","\n","## Create a TimeSeriesDataset instance and initialize DataLoader for each data\n","value_train = {key: main_train_dict['scaled_value_seq_dict'].get(key, []) +\\\n","  censored_train_dict['scaled_value_seq_dict'].get(key, []) \\\n","    for key in set(main_train_dict['scaled_value_seq_dict']) | set(censored_train_dict['scaled_value_seq_dict'])}\n","cov_train = main_train_dict['cov_seq'] + censored_train_dict['cov_seq']\n","value_train_np = value_dict_to_np(value_train, [TARGET_TASK]) # shape: (num_groups, seq_len, num_tasks)\n","train_dataset = CrossSectionalTimeSeriesDataset(value_train_np, cov_train, hyperparam.INPUT_CHUNK_LENGTH)\n","\n","## To be iterated over batches of data during training\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=hyperparam.BATCH_SIZE, # how many samples per batch to load\n","                          shuffle=True, # have the data reshuffled at every epoch to reduce model overfitting\n","                          drop_last=False, # If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller\n","                          collate_fn=collate_fn,\n","                          pin_memory=True, # True for faster data transfer to GPUs, False if out of memory\n","                          num_workers=multiprocessing.cpu_count()//2,\n","                        #   num_workers=hyperparam.NUM_WORKERS, # how many subprocesses to use for data loading. (0: loaded in the main process)\n","                          )\n","\n","if hyperparam.VAL_LOSS:\n","    value_valid = {key: main_val_dict['scaled_value_seq_dict'].get(key, []) +\\\n","        censored_val_dict['scaled_value_seq_dict'].get(key, []) \\\n","            for key in set(main_val_dict['scaled_value_seq_dict']) | set(censored_val_dict['scaled_value_seq_dict'])}\n","    cov_valid = main_val_dict['cov_seq'] + censored_val_dict['cov_seq']\n","    value_valid_np = value_dict_to_np(value_valid, [TARGET_TASK]) # shape: (num_groups, seq_len, num_tasks)\n","    val_dataset = CrossSectionalTimeSeriesDataset(value_valid_np, cov_valid, hyperparam.INPUT_CHUNK_LENGTH)\n","    val_loader = DataLoader(val_dataset, batch_size=hyperparam.BATCH_SIZE, shuffle=False, drop_last=False, # no shuffle for validation\n","                              collate_fn=collate_fn,\n","                              )\n","\n","## get the dimension of the target and covariate data\n","first_sample = next(iter(train_dataset)) # or train_dataset[0]\n","first_batch = next(iter(train_loader))\n","\n","## number of targets, number of covariate features\n","tgt_dim, cov_dim = first_sample[\"target\"].shape[1], first_sample[\"covariate\"].shape[1] # 3 , 60\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","\n","If considering attention mask later, modify with this:\n","- `def forward(self, src, attention_mask):`\n","- `x = self.transformer(src=src, tgt=tgt, src_key_padding_mask=attention_mask)`"]},{"cell_type":"markdown","metadata":{},"source":["## Training (Estimation)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## For memory monitoring ==\n","# !pip install memory_profiler\n","# %load_ext memory_profiler\n","# %memit my_function()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":681},"executionInfo":{"elapsed":84181,"status":"ok","timestamp":1698252575062,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"uV1LoUeqGPD3","outputId":"8877429e-db98-4999-bac5-dfc60b0e03f6"},"outputs":[],"source":["from libs.models import STL_Transformer\n","\n","## initialize model\n","model = STL_Transformer(\n","    input_dim=tgt_dim + cov_dim,\n","    feature_dict=covariate_name_to_index, \n","    num_merchant=len(MERCHANT_NAMES_EMB_INT),\n",").to(device)\n","\n","## define optimizer and loss criterion\n","optimizer = torch.optim.Adam(model.parameters(), lr=hyperparam.LEARNING_RATE)\n","loss_criterion = torch.nn.MSELoss()\n","\n","## initialize empty list for losses and early stop\n","train_losses, valid_losses = [], []\n","pre_valid_loss, cnt_no_improve = np.inf, 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in tqdm(range(hyperparam.N_EPOCHS), desc=\"training\", unit=\"epoch\"):\n","    train_loss = 0.0 # within each epoch, initialize train loss to 0\n","\n","    for target_input, cov_input, gt in train_loader:\n","        optimizer.zero_grad() # reset optimizer gradients to zero\n","        train_input = torch.cat((target_input,cov_input), dim=-1).to(device)   \n","        next_value = model(train_input) # forward pass data through the model\n","        loss = loss_criterion(next_value, gt.to(device)) # calculate and update loss \n","        train_loss += loss.item() # accumulate batch loss within each epoch\n","        loss.backward() # backpropagation\n","        _ = nn.utils.clip_grad_norm_(model.parameters(), hyperparam.GRADCLIP) # clip gradients to prevent exploding gradients\n","        optimizer.step() # update parameters based on gradients\n","\n","    train_losses.append(train_loss/len(train_loader)) # append total train loss for each epoch\n","\n","    if hyperparam.VAL_LOSS:\n","        valid_loss = 0.0\n","        for target_input, cov_input, gt in val_loader:\n","            valid_input = torch.cat((target_input,cov_input), dim=-1).to(device)\n","            next_value = model(valid_input)\n","            loss = loss_criterion(next_value, gt.to(device))\n","            valid_loss += loss.item()\n","\n","        valid_losses.append(valid_loss/len(val_loader))\n","        \n","        # Early stop evaluate\n","        if pre_valid_loss - valid_loss  < hyperparam.MINDELTA:\n","            cnt_no_improve += 1\n","            if cnt_no_improve > hyperparam.PATIENCE:\n","                break\n","        else:\n","            cnt_no_improve = 0\n","            \n","        pre_valid_loss = valid_loss\n","    \n","    if epoch % 5 == 0:\n","        print(\"train_loss:{:.4f}\".format(train_loss))\n","        print(\"val loss: {:.4f}\".format(valid_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_losses():\n","    plt.figure(figsize=(10, 6))\n","    plt.xlabel(\"# of epoch\")\n","    plt.plot(train_losses, label=\"train loss\")\n","    plt.plot(valid_losses, label=\"valid loss\")\n","    plt.title(f\"{PREDICTION_GOAL} Embedding {len(MERCHANT_NAMES_EMB_INT)} model Losses\")\n","    plt.legend()\n","        \n","plot_losses()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","if SAVE_MODE:\n","    # To save the model\n","    with open(f'{SAVE_MODEL}/{PREDICTION_GOAL}_embedding_{len(MERCHANT_NAMES_EMB_INT)}.pkl', 'wb') as file:\n","        pickle.dump(model, file)\n","\n","    # # To load the model\n","    # with open(f'{SAVE_MODEL}/{PREDICTION_GOAL}_embedding_{len(MERCHANT_NAMES_EMB_INT)}.pkl', 'rb') as file:\n","    #     model = pickle.load(file)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prediction (Inference)\n","\n","- Rolling forecast origin or walk-forward validation (which means generating predictions one step at a time and conditioning upon the predicted values)\n","\n","For each rolling window:\n","- Use the last `INPUT_CHUNK_LENGTH` weeks of data as input to forecast the next week.\n","- Append the forecasted value to the actual data.\n","- Move the window one week forward and repeat.\n","\n","*NOTE:* Error can be accumulated in **triple** way as we now take acq_hat, repeat order per customer_hat, aov_hat all together for next acq prediction,for example."]},{"cell_type":"markdown","metadata":{},"source":["### Cohort 1 (acquired after the beginning of train period)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.forecasting import prepare_TimeSeriesDataset, rolling_forecast_stl\n","\n","# Sets the module in evaluation mode\n","model.eval()\n","\n","# rolling forecast (scaled)\n","main_value_test_np = value_dict_to_np(main_test_dict['scaled_value_seq_dict'], [TARGET_TASK])\n","test_datasets = prepare_TimeSeriesDataset(main_value_test_np, main_test_dict['cov_seq'], \n","                                          CrossSectionalTimeSeriesDataset, hyperparam.INPUT_CHUNK_LENGTH)\n","main_value_test_pred = rolling_forecast_stl(test_datasets, model, device, TARGET_TASK)\n","\n","# get ground truth in the test period\n","main_df_test_net = main_df_test[main_df_test['time']>=hyperparam.TEST_START][main_df_test['tenure']>=0]\n","actual_main = main_df_test_net[['group', 'time'] + [TARGET_TASK]]\n","\n","# scale back the forecast\n","predicted_main = util.inverse_scale_np_to_dataframe_embedding(main_test_dict['scaler'], main_test_dict['group_seq'], main_test_dict['time_seq'], \n","                                                                  main_value_test_pred, hyperparam.INPUT_CHUNK_LENGTH)\n","predicted_main.sort_values(['group', 'time'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.plots import plot_time_series_multiple\n","\n","def plot_ropc_cohort1(merchant_name):\n","    selected_tuples = [t for t in main_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 3 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","        group = selected_tuples[group_index][1]\n","        plot_time_series_multiple(\n","            main_df[(main_df['group'] == group) & (main_df['merchant_name'] == merchant_name)], \n","            predicted_main[(predicted_main['group'] == group) & (predicted_main['merchant_name'] == merchant_name)],\n","            'time', TARGET_TASK, \n","            title=f'{merchant_name} [Group {group}\\'s Repeat Order per Customer]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","selected_merchant_name = list(MERCHANT_NAMES_EMB_INT.keys())[0]  # assuming MERCHANT_NAMES_EMB_IDX is a dictionary\n","plot_ropc_cohort1(selected_merchant_name)"]},{"cell_type":"markdown","metadata":{},"source":["### Cohort 0 (acquired before the beginning of train period)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sets the module in evaluation mode\n","model.eval()\n","\n","# rolling forecast (scaled)\n","censored_value_test_np = value_dict_to_np(censored_test_dict['scaled_value_seq_dict'], [TARGET_TASK])\n","censored_test_datasets = prepare_TimeSeriesDataset(censored_value_test_np, censored_test_dict['cov_seq'], \n","                                                   CrossSectionalTimeSeriesDataset, hyperparam.INPUT_CHUNK_LENGTH)\n","censored_value_test_pred = rolling_forecast_stl(censored_test_datasets, model, device, TARGET_TASK)\n","\n","# get ground truth in the test period\n","censored_df_test_net = censored_df_test[censored_df_test['time']>=hyperparam.TEST_START][censored_df_test['tenure']>=0]\n","actual_censored = censored_df_test_net[['merchant_name', 'group', 'time'] + [TARGET_TASK]]\n","\n","# scale back the forecast\n","predicted_censored = util.inverse_scale_np_to_dataframe_embedding(censored_test_dict['scaler'], censored_test_dict['group_seq'], censored_test_dict['time_seq'], \n","                                                                  censored_value_test_pred, hyperparam.INPUT_CHUNK_LENGTH)\n","predicted_censored.sort_values(['group', 'time'], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### Repeat order per customer plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_ropc_cohort0(merchant_name):\n","    selected_tuples = [t for t in censored_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 3 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","        group = selected_tuples[group_index][1]\n","        plot_time_series_multiple(\n","            actual_censored[(actual_censored['group'] == group) & (actual_censored['merchant_name'] == merchant_name)], \n","            predicted_censored[(predicted_censored['group'] == group) & (predicted_censored['merchant_name'] == merchant_name)],\n","            'time', TARGET_TASK, \n","            title=f'{merchant_name} [Group {group}\\'s Repeat Order per Customer]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","selected_merchant_name = list(MERCHANT_NAMES_EMB_INT.keys())[0]  # assuming MERCHANT_NAMES_EMB_IDX is a dictionary\n","plot_ropc_cohort0(selected_merchant_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.plots import save_plots_to_pdf\n","\n","if SAVE_MODE:\n","    # Saving traininig and validation losses\n","    save_plots_to_pdf([\n","        (plot_losses, {}),\n","        ], f'{SAVE_EPOCH}/{PREDICTION_GOAL}_embedding_{len(MERCHANT_NAMES_EMB_INT)}_loss.pdf')\n","    \n","    predicted_combined = pd.concat([predicted_censored, predicted_main])\n","    actual_combined = pd.concat([actual_censored, actual_main])\n","\n","    for key in MERCHANT_NAMES_EMB_INT.keys():\n","        MERCHANT_NAME = key    \n","        predicted_main_sub = predicted_combined[predicted_combined.merchant_name == MERCHANT_NAME]\n","        predicted_main_sub.to_csv(f'{SAVE_PREDICT}/{MERCHANT_NAME}_pred.csv', index=False)\n","        actual_combined_sub = actual_combined[actual_combined.merchant_name == MERCHANT_NAME]        \n","        actual_combined_sub.to_csv(f'{SAVE_PREDICT}/{MERCHANT_NAME}_pred.csv', index=False)\n","        \n","        # Saving plots to PDF\n","        save_plots_to_pdf([\n","            (plot_ropc_cohort1, {'merchant_name': f'{MERCHANT_NAME}'}),\n","            (plot_ropc_cohort0, {'merchant_name': f'{MERCHANT_NAME}'}),\n","            ], f'{SAVE_PLOT}/{MERCHANT_NAME}.pdf')\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPxURsHtawxcGy64/iEvkgk","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
