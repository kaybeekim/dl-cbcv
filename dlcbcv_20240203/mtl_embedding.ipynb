{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MTL-Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"elapsed":3876,"status":"ok","timestamp":1698252479641,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"zSkLguiJA2AI","outputId":"cf9ab9c4-7300-4a99-ef16-350bf81ed5a4"},"outputs":[],"source":["import libs.config as config\n","import libs.hyperparam as hyperparam\n","import libs.util as util \n","import libs.customized_dataset as customized_dataset\n","import libs.models as models \n","import libs.forecasting as forecasting \n","import libs.plots as plots \n","\n","import os  # for interacting with the operating system\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # for debugging cuda errors\n","import glob  # for finding files in directories\n","import warnings \n","warnings.filterwarnings('ignore') # for ignoring all warnings\n","# import argparse  # for parsing command line arguments (for running from the terminal)\n","\n","import random\n","import math # for math operations\n","import time  # for time-related functionalities\n","import holidays # for checking if a date is a holiday\n","import matplotlib.pyplot as plt  # for plotting\n","plt.set_cmap('cividis') # color map for the plots to 'cividis'\n","## Enable the display of matplotlib plots inline in a Jupyter notebook\n","%matplotlib inline \n","import matplotlib.ticker as ticker # for customizing the plots' tick locations and labels\n","import numpy as np  # for numerical computations\n","import pandas as pd  # for data manipulation and analysis\n","pd.set_option('display.max_columns', None) # to display all columns\n","from datetime import date, datetime, timedelta  # for working with dates and times\n","\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler, Normalizer  # for scaling data\n","from tqdm import tqdm  # for creating progress bars\n","\n","import torch  # for building and training neural networks\n","from torch.utils.data import Dataset, DataLoader # for loading and managing datasets\n","import torch.nn as nn  # for building neural networks\n","import torch.nn.functional as F  # for implementing various activation functions\n","import torch.optim as optim  # for defining optimizers\n","\n","# set the device as GPU with index 0\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","config.get_environ_info(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Check NVIDIA GPU"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["### Set data path"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698252479642,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"N8LI4FloRwjj"},"outputs":[],"source":["## Google cluster\n","# READ_DIR = r\"/home/kkim476/dl-cbcv/data/weekly_cohort_data_1000_missing_filled_final\"\n","READ_DIR = r\"/home/kkim476/dl-cbcv/data/selected_ten\"\n","WORK_DIR = r\"/home/kkim476/dl-cbcv\""]},{"cell_type":"markdown","metadata":{},"source":["### Set prediction target, covariates, and save mode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set prediction target\n","PREDICTION_GOAL = 'mtl_4tasks' # mtl_3tasks or mtl_4tasks which includes sales binding \n","TARGET_TASKS = config.get_target_variable_name(PREDICTION_GOAL)\n","\n","if PREDICTION_GOAL == 'mtl_4tasks':\n","    weights = torch.tensor([hyperparam.w_acq, hyperparam.w_ropc, hyperparam.w_aov, hyperparam.w_spend]).to(device)\n","elif PREDICTION_GOAL == 'mtl_3tasks':\n","    weights = torch.tensor([hyperparam.w_acq, hyperparam.w_ropc, hyperparam.w_aov]).to(device)\n","\n","### set covariate features\n","USE_EMBEDDING = True\n","COHORT_EMBEDDING = True\n","DUMMY_VAR = False # entity embedding, rather than one hot encoding\n","\n","### folder path for saving results\n","SAVE_MODE = False\n","SAVE_DIR = f'{WORK_DIR}/results/{PREDICTION_GOAL}_embedding10_{datetime.today().strftime(\"%Y-%m-%d\")}'\n","SAVE_MODEL, SAVE_EPOCH, SAVE_PLOT, SAVE_PREDICT, SAVE_ACTUAL = config.create_save_folders(SAVE_MODE, SAVE_DIR)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1698252480142,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"4lyMoWePRwdA","outputId":"19e39ca9-e4c8-4947-e7d2-10623afba5ef"},"outputs":[],"source":["## this file is cohort-week level aggregated panel data after ETL of raw earnest transaction DB\n","raw_df, MERCHANT_NAMES_EMB_INT = util.read_files_generate_behaviorfeatures_get_embed_dict(\n","    READ_DIR, hyperparam.TRAIN_START, hyperparam.TEST_START, hyperparam.TEST_END,\n","    group_identifier='acq_week', time_identifier='week',\n","    acquisition_identifier='N_week_cohort',\n","    order_identifier = 'orders',\n","    spend_identifier = 'spend')\n","\n","FREQ, week_start = util.get_week_start(raw_df, hyperparam.TRAIN_START, hyperparam.TEST_START, hyperparam.TEST_END) "]},{"cell_type":"markdown","metadata":{},"source":["### Zero padding\n","\n","This is technical but important piece regarding how to handle 'cohort' triangle data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":1293,"status":"ok","timestamp":1698252481771,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"C9sJ6-lURwVW","outputId":"548f3b42-55ce-4d91-9be9-253beb866318"},"outputs":[],"source":["# columns that I want to keep\n","company_static = ['merchant_index', 'merchant', 'parent_merchant', 'category', 'subcategory']\n","if USE_EMBEDDING:\n","    company_static = company_static + ['merchant_emb_int', 'merchant_name']\n","company_dynamic = ['cohort_size', 'orders', 'spend',\n","                #    'active_users', \n","                #    'rpt_orders','initial_order',\n","                #    'rpt_spend','initial_spend',\n","                #    'initial_aov','rpt_aov',\n","                   ]\n","# company_dynamic = []\n","\n","assert all(column in list(raw_df.columns) for column in company_static + company_dynamic), \\\n","    'some columns are missing in the raw_df'\n","\n","df_padded = util.zero_padding(raw_df, TARGET_TASKS, company_static, company_dynamic, hyperparam.INPUT_CHUNK_LENGTH, FREQ,\n","                               use_merchant_embedding=True, merchant_name='merchant_name')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate calendar time covariates\n","\n","Our covariates include:\n","- week of year_t+1 (1 week ahead) : (categorical) one-hot encoding. 53 variables.\n","- holidays_t+1 (1 week ahead) : (binary) dummy variable. 1 variable.\n","- global trend_t : (continuous) quadratic. 2 variables.\n","- cohort numbering_i : (continuous) quadratic. 2 variables.\n","- cohort tenure_it : (continuous) quadratic. 2 variables."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698252482117,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"dNrriuNBY7zp","outputId":"8344e916-0226-4c0c-f047-777bcf15d2d7"},"outputs":[],"source":["df_padded_w_cov, country_holidays = util.generate_calendartime_features(df_padded, FREQ, week_start, DUMMY_VAR)\n","# util.check_holidays(df_padded_w_cov, country_holidays)\n","df_padded_w_cov = util.generate_cohort_features(df_padded_w_cov, TARGET_TASKS, COHORT_EMBEDDING, DUMMY_VAR, hyperparam.COHORT_EMB_NUM)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## limit data range\n","df = df_padded_w_cov[df_padded_w_cov['group']<=hyperparam.TEST_END] # limit cohort\n","df = df[df['time']>=hyperparam.TRAIN_START_with_offset][(df['time']<=hyperparam.TEST_END)] # limit time window\n","\n","## get covariate feature names\n","COVARIATE_FEATURE_NAMES, covariate_name_to_index = config.get_covariate_variable_name(df, USE_EMBEDDING, COHORT_EMBEDDING, DUMMY_VAR)\n","print(COVARIATE_FEATURE_NAMES)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del df_padded\n","del df_padded_w_cov\n","del country_holidays"]},{"cell_type":"markdown","metadata":{},"source":["### Split into train:val:test (time-wise) X censored:uncensored (group-wise)\n","\n","<img src=\"../img/cohort_triangle.png\" alt=\"cohort_triangle\" width=\"600\" height=\"400\">\n","\n","For censored (cohort 0) group of cohorts (who acquired in 2016)\n","- (A) train: ~ 2018\n","- (B) validation : '2019-01-01' ~ '2019-03-31'\n","- (C) test : '2019-04-01' ~\n","\n","For uncensored (cohort 1~) group of cohorts (who acquired in 2017~)\n","- (D) train: ~ 2018\n","- (E) validation : '2019-01-01' ~ '2019-03-31'\n","- (F) test : '2019-04-01' ~\n","\n","We do not distinguish between censored and uncensored cohorts in training. We found empirically that it is better to train them together by obtaining more samples at the cost of giving up incorporating heterogeneity between two.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698252483347,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"UUc0EZm2Y7hb"},"outputs":[],"source":["(main_df, main_df_train, main_df_valid, main_df_test, \n"," censored_df, censored_df_train, censored_df_valid, censored_df_test) = util.split_dataframe(\n","    df, hyperparam.TRAIN_START, hyperparam.VAL_START, hyperparam.TEST_START, \n","    hyperparam.VAL_START_with_offset, hyperparam.TEST_START_with_offset, hyperparam.VAL_LOSS)"]},{"cell_type":"markdown","metadata":{},"source":["### Transform data frame to scaled numpy array\n","\n","1. As numpy array is more efficient to handle, we transform pandas data frame into numpy array.\n","\n","2. For each task, we scale the data with its own scaler. This is important for multi-task learning as each task has different scale."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## entire sequences (unscaled)\n","whole_dict = util.df_to_numpy(df, TASKS=TARGET_TASKS, COVARIATES=COVARIATE_FEATURE_NAMES, \n","                              use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","\n","## main cohorts' sequences\n","main_train_dict = util.df_to_scaled_numpy(main_df_train, TARGET_TASKS, COVARIATE_FEATURE_NAMES,\n","                                          use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","main_test_dict = util.df_to_scaled_numpy(main_df_test, TARGET_TASKS, COVARIATE_FEATURE_NAMES,  main_train_dict['scaler'],\n","                                         use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","if hyperparam.VAL_LOSS:\n","    main_val_dict = util.df_to_scaled_numpy(main_df_valid, TARGET_TASKS, COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                            use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","\n","## censored cohorts' sequences\n","censored_train_dict = util.df_to_scaled_numpy(censored_df_train, TARGET_TASKS, COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                              use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","censored_test_dict = util.df_to_scaled_numpy(censored_df_test, TARGET_TASKS, COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                             use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n","if hyperparam.VAL_LOSS:\n","    censored_val_dict = util.df_to_scaled_numpy(censored_df_valid, TARGET_TASKS, COVARIATE_FEATURE_NAMES, main_train_dict['scaler'],\n","                                                use_merchant_embedding=USE_EMBEDDING, merchant_name='merchant_name')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","if SAVE_MODE:\n","    # To save\n","    with open(f'{SAVE_MODEL}/np_dict_data.pkl', 'wb') as file:\n","        pickle.dump([whole_dict, \n","                    main_train_dict, main_test_dict, main_val_dict,\n","                    censored_train_dict, censored_test_dict, censored_val_dict], file)\n","\n","    df.to_hdf(f'{SAVE_MODEL}/df.h5', key='df', mode='w')\n","    \n","    # # To load\n","    # with open(f'{SAVE_MODEL}/np_dict_data.pkl', 'rb') as file:\n","    #     whole_dict, main_train_dict, main_test_dict, main_val_dict, \\\n","    #         censored_train_dict, censored_test_dict, censored_val_dict = pickle.load(file)\n","            \n","    # df = pd.read_hdf(f'{SAVE_MODEL}/df.h5', key='df')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and DataLoader\n","the creation of custom datasets, and the initialization of the DataLoader.\n","- custom dataset class will group the data by the group column\n","- collate function will handle the padding and attention masking\n","\n","<img src=\"../img/data%20period%20and%20samples.png\" alt=\"data_period\" width=\"600\" height=\"400\">\n","\n","<img src=\"../img/input%20output%20format.png\" alt=\"input_output\" width=\"600\" height=\"400\">\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate train loader and validation loader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1698252486506,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"tv2X8ubGV2YZ"},"outputs":[],"source":["from libs.customized_dataset import CrossSectionalTimeSeriesDataset, collate_fn, value_dict_to_np\n","import multiprocessing\n","\n","## Create a TimeSeriesDataset instance and initialize DataLoader for each data\n","value_train = {key: main_train_dict['scaled_value_seq_dict'].get(key, []) +\\\n","  censored_train_dict['scaled_value_seq_dict'].get(key, []) \\\n","    for key in set(main_train_dict['scaled_value_seq_dict']) | set(censored_train_dict['scaled_value_seq_dict'])}\n","cov_train = main_train_dict['cov_seq'] + censored_train_dict['cov_seq']\n","value_train_np = value_dict_to_np(value_train, TARGET_TASKS) # shape: (num_groups, seq_len, num_tasks)\n","train_dataset = CrossSectionalTimeSeriesDataset(value_train_np, cov_train, hyperparam.INPUT_CHUNK_LENGTH)\n","\n","## To be iterated over batches of data during training\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=hyperparam.BATCH_SIZE, # how many samples per batch to load\n","                          shuffle=True, # have the data reshuffled at every epoch to reduce model overfitting\n","                          drop_last=False, # If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller\n","                          collate_fn=collate_fn,\n","                          pin_memory=True, # True for faster data transfer to GPUs, False if out of memory\n","                          num_workers=multiprocessing.cpu_count()//2,\n","                        #   num_workers=hyperparam.NUM_WORKERS, # how many subprocesses to use for data loading. (0: loaded in the main process)\n","                          )\n","\n","if hyperparam.VAL_LOSS:\n","    value_valid = {key: main_val_dict['scaled_value_seq_dict'].get(key, []) +\\\n","        censored_val_dict['scaled_value_seq_dict'].get(key, []) \\\n","            for key in set(main_val_dict['scaled_value_seq_dict']) | set(censored_val_dict['scaled_value_seq_dict'])}\n","    cov_valid = main_val_dict['cov_seq'] + censored_val_dict['cov_seq']\n","    value_valid_np = value_dict_to_np(value_valid, TARGET_TASKS) # shape: (num_groups, seq_len, num_tasks)\n","    val_dataset = CrossSectionalTimeSeriesDataset(value_valid_np, cov_valid, hyperparam.INPUT_CHUNK_LENGTH)\n","    val_loader = DataLoader(val_dataset, batch_size=hyperparam.BATCH_SIZE, shuffle=False, drop_last=False, # no shuffle for validation\n","                              collate_fn=collate_fn,\n","                              )\n","\n","## get the dimension of the target and covariate data\n","first_sample = next(iter(train_dataset)) # or train_dataset[0]\n","first_batch = next(iter(train_loader))\n","\n","## number of targets, number of covariate features\n","tgt_dim, cov_dim = first_sample[\"target\"].shape[1], first_sample[\"covariate\"].shape[1] # 3 , 60\n","if PREDICTION_GOAL == 'mtl_4tasks':\n","    tgt_dim = 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if SAVE_MODE:\n","    # To save the model\n","    with open(f'{SAVE_MODEL}/data_loader.pkl', 'wb') as file:\n","        pickle.dump([train_loader, val_loader], file)\n","        \n","    # with open(f'{SAVE_MODEL}/data_loader.pkl', 'rb') as file:\n","    #     train_loader, val_loader = pickle.load(file)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","\n","<div style=\"background-color:white;\">\n","    <img src=\"../img/MTL.png\" alt=\"mtl\" width=\"550\" height=\"700\">\n","</div>\n","\n","If considering attention mask later, modify with this:\n","- `def forward(self, src, attention_mask):`\n","- `x = self.transformer(src=src, tgt=tgt, src_key_padding_mask=attention_mask)`"]},{"cell_type":"markdown","metadata":{},"source":["## Training (Estimation)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## For memory monitoring ==\n","# !pip install memory_profiler\n","# %load_ext memory_profiler\n","# %memit my_function()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":681},"executionInfo":{"elapsed":84181,"status":"ok","timestamp":1698252575062,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"uV1LoUeqGPD3","outputId":"8877429e-db98-4999-bac5-dfc60b0e03f6"},"outputs":[],"source":["from libs.models import MTL_Transformer, calculate_MTLloss_3tasks, calculate_MTLloss_4tasks\n","\n","## initialize model\n","model = MTL_Transformer(\n","    input_dim=tgt_dim + cov_dim,\n","    feature_dict=covariate_name_to_index, \n","    d_model=hyperparam.D_MODEL_MTL_EMB1,\n","    num_encoder_layers=hyperparam.N_ENCODER_LAYERS_MTL_EMB1,\n","    num_decoder_layers=hyperparam.N_DECODER_LAYERS_MTL_EMB1,\n","    d_feedforward=hyperparam.D_FEEDFORWARD_MTL_EMB1,\n","    d_feedforward_task=hyperparam.D_FEEDFORWARD_TASK_MTL_EMB1,\n","    dropout=hyperparam.DROPOUT_MTL_EMB1,\n","    num_merchant=len(MERCHANT_NAMES_EMB_INT),\n",").to(device)\n","\n","## define optimizer and loss criterion\n","optimizer = torch.optim.Adam(model.parameters(), lr=hyperparam.LEARNING_RATE_MTL_EMB1)\n","individual_loss_criterion = torch.nn.MSELoss(reduction='none')\n","\n","## initialize empty list for losses and early stop\n","train_losses, valid_losses = [], []\n","pre_valid_loss, cnt_no_improve = np.inf, 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if PREDICTION_GOAL == 'mtl_3tasks':\n","\n","    for epoch in tqdm(range(hyperparam.N_EPOCHS), desc=\"training\", unit=\"epoch\"):\n","        train_loss = 0.0 # within each epoch, initialize train loss to 0\n","\n","        for target_input, cov_input, gt in train_loader:\n","            optimizer.zero_grad() # reset optimizer gradients to zero\n","            loss = calculate_MTLloss_3tasks(model, target_input, cov_input, gt, \n","                                    individual_loss_criterion, weights, device)\n","            train_loss += loss.item() # accumulate batch loss within each epoch\n","            loss.backward() # backpropagation\n","            _ = nn.utils.clip_grad_norm_(model.parameters(), hyperparam.GRADCLIP) # clip gradients to prevent exploding gradients\n","            optimizer.step() # update parameters based on gradients\n","\n","        train_losses.append(train_loss/len(train_loader)) # append total train loss for each epoch\n","\n","        if hyperparam.VAL_LOSS:\n","            valid_loss = 0.0\n","            for target_input, cov_input, gt in val_loader:\n","                loss = calculate_MTLloss_3tasks(model, target_input, cov_input, gt, \n","                                                individual_loss_criterion, weights, device)\n","                valid_loss += loss.item()\n","\n","            valid_losses.append(valid_loss/len(val_loader))\n","            \n","            # Early stop evaluate\n","            if pre_valid_loss - valid_loss  < hyperparam.MINDELTA:\n","                cnt_no_improve += 1\n","                if cnt_no_improve > hyperparam.PATIENCE:\n","                    break\n","            else:\n","                cnt_no_improve = 0\n","                pre_valid_loss = valid_loss\n","        \n","        if epoch % 10 == 0:\n","            print(\"train_loss:{:.4f}\".format(train_loss))\n","            if hyperparam.VAL_LOSS:\n","                print(\"val loss: {:.4f}\".format(valid_loss))\n","            \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if PREDICTION_GOAL == 'mtl_4tasks':\n","    \n","    ## get min and max values for each target\n","    min_scaler_values = {target: {} for target in TARGET_TASKS}\n","    max_scaler_values = {target: {} for target in TARGET_TASKS}\n","    for target in TARGET_TASKS:\n","        min_scaler_values[target] = torch.tensor(main_train_dict['scaler'][target].data_min_, dtype=torch.float32).to(device)\n","        max_scaler_values[target] = torch.tensor(main_train_dict['scaler'][target].data_max_, dtype=torch.float32).to(device)\n","\n","    ## trainig loop\n","    for epoch in tqdm(range(hyperparam.N_EPOCHS), desc=\"training\", unit=\"epoch\"):\n","        train_loss = 0.0 # within each epoch, initialize train loss to 0\n","\n","        for target_input, cov_input, gt in train_loader:   \n","            optimizer.zero_grad() # reset optimizer gradients to zero\n","            loss = calculate_MTLloss_4tasks(model, target_input, cov_input, gt, \n","                                            max_scaler_values, min_scaler_values, TARGET_TASKS,\n","                                            individual_loss_criterion, weights, device)\n","            train_loss += loss.item() # accumulate batch loss within each epoch\n","            loss.backward() # backpropagation\n","            _ = nn.utils.clip_grad_norm_(model.parameters(), hyperparam.GRADCLIP) # clip gradients to prevent exploding gradients\n","            optimizer.step() # update parameters based on gradients\n","\n","        train_losses.append(train_loss/len(train_loader)) # append total train loss for each epoch\n","\n","        if hyperparam.VAL_LOSS:\n","            valid_loss = 0.0\n","            for target_input, cov_input, gt in val_loader:\n","                loss = calculate_MTLloss_4tasks(model, target_input, cov_input, gt, \n","                                                max_scaler_values, min_scaler_values, TARGET_TASKS,\n","                                                individual_loss_criterion, weights, device)\n","                valid_loss += loss.item()\n","\n","            valid_losses.append(valid_loss/len(val_loader))\n","            \n","            # Early stop evaluate\n","            if pre_valid_loss - valid_loss  < hyperparam.MINDELTA:\n","                cnt_no_improve += 1\n","                if cnt_no_improve > hyperparam.PATIENCE:\n","                    break\n","            else:\n","                cnt_no_improve = 0\n","                \n","            pre_valid_loss = valid_loss\n","        \n","        if epoch % 10 == 0:\n","            print(\"train_loss:{:.4f}\".format(train_loss))\n","            if hyperparam.VAL_LOSS:\n","                print(\"val loss: {:.4f}\".format(valid_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_losses():\n","    plt.figure(figsize=(10, 6))\n","    plt.xlabel(\"# of epoch\")\n","    plt.plot(train_losses, label=\"train loss\")\n","    plt.plot(valid_losses, label=\"valid loss\")\n","    plt.title(f\"{PREDICTION_GOAL} Embedding {len(MERCHANT_NAMES_EMB_INT)} model Losses\")\n","    plt.legend()\n","        \n","plot_losses()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","if SAVE_MODE:\n","    # To save the model\n","    with open(f'{SAVE_MODEL}/{PREDICTION_GOAL}_embedding_{len(MERCHANT_NAMES_EMB_INT)}.pkl', 'wb') as file:\n","        pickle.dump(model, file)\n","\n","    # # To load the model\n","    # with open(f'{SAVE_MODEL}/{PREDICTION_GOAL}_embedding_{len(MERCHANT_NAMES_EMB_INT)}.pkl', 'rb') as file:\n","    #     model = pickle.load(file)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prediction (Inference)\n","\n","- Rolling forecast origin or walk-forward validation (which means generating predictions one step at a time and conditioning upon the predicted values)\n","\n","For each rolling window:\n","- Use the last `INPUT_CHUNK_LENGTH` weeks of data as input to forecast the next week.\n","- Append the forecasted value to the actual data.\n","- Move the window one week forward and repeat.\n","\n","*NOTE:* Error can be accumulated in **triple** way as we now take acq_hat, repeat order per customer_hat, aov_hat all together for next acq prediction,for example."]},{"cell_type":"markdown","metadata":{},"source":["### Cohort 1 (acquired after the beginning of train period)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.forecasting import prepare_TimeSeriesDataset, rolling_forecast, get_sales_recovered_data\n","\n","# Sets the module in evaluation mode\n","model.eval()\n","\n","# rolling forecast (scaled)\n","main_value_test_np = value_dict_to_np(main_test_dict['scaled_value_seq_dict'], TARGET_TASKS)\n","test_datasets = prepare_TimeSeriesDataset(main_value_test_np, main_test_dict['cov_seq'], CrossSectionalTimeSeriesDataset, hyperparam.INPUT_CHUNK_LENGTH)\n","main_value_test_pred = rolling_forecast(test_datasets, model, device, TARGET_TASKS, verbose=True)\n","\n","# get ground truth in the test period\n","main_df_test_net = main_df_test[main_df_test['time']>=hyperparam.TEST_START][main_df_test['tenure']>=0]\n","actual_main = main_df_test_net[['merchant_name','group', 'time', 'cohort_size'] + TARGET_TASKS]\n","\n","# scale back the forecast\n","predicted_main = util.inverse_scale_np_to_dataframe_embedding(main_test_dict['scaler'], main_test_dict['group_seq'], main_test_dict['time_seq'], \n","                                                                  main_value_test_pred, hyperparam.INPUT_CHUNK_LENGTH)\n","predicted_main.sort_values(['merchant_name', 'group', 'time'], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### Cohort 1's Sales recovery\n","\n","For sales prediction over the holdout period, we need both acquisition over the calibration period and acquisition over the holdout period. (because we need to scale up repeat order per customer from the cohort who were acquired prior to holdout period)\n","\n","We only have predicted acquisition values for holdout period. We imputed acquisition values for calibration period with actual acquisition value. This makes sense, as our goal is doing our best to maximize our prediction performance given observed calibration data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if PREDICTION_GOAL == 'mtl_4tasks':\n","    acq_variable, ropc_variable, aov_variable, _ = TARGET_TASKS\n","elif PREDICTION_GOAL == 'mtl_3tasks':\n","    acq_variable, ropc_variable, aov_variable = TARGET_TASKS\n","else:\n","    raise ValueError('PREDICTION_GOAL is not tracked.')\n","\n","## get the average of pseudo acquisition\n","actual_main_acq = actual_main[['merchant_name','time', acq_variable]].drop_duplicates().reset_index(drop=True)\n","predicted_main_acq = predicted_main[['merchant_name','time', acq_variable]].groupby(['merchant_name','time']).mean().reset_index()\n","\n","actual_main, predicted_main = get_sales_recovered_data(predicted_main_acq, actual_main, predicted_main, hyperparam.TEST_START,\n","                                                       USE_EMBEDDING, MERCHANT_NAMES_EMB_INT)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Acquisition plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.plots import plot_time_series, plot_time_series_multiple\n","def plot_acq_cohort1(merchant_name):\n","    plot_time_series(actual_main_acq[actual_main_acq['merchant_name']==merchant_name], \n","                     predicted_main_acq[actual_main_acq['merchant_name']==merchant_name],\n","                     'time', acq_variable,\n","        title=f'{merchant_name} [Uncensored Cohort 1\\'s Acquisition]',\n","        )\n","\n","plot_acq_cohort1(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["#### Repeat order per customer plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_ropc_cohort1(merchant_name):\n","    selected_tuples = [t for t in main_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 5 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","       group = selected_tuples[group_index][1]\n","       plot_time_series_multiple(\n","            main_df[(main_df['group'] == group) & (main_df['merchant_name'] == merchant_name)], \n","            predicted_main[(predicted_main['group'] == group) & (predicted_main['merchant_name'] == merchant_name)],\n","            'time', ropc_variable, \n","            title=f'{merchant_name} [Group {group}\\'s Repeat Order per Customer]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","plot_ropc_cohort1(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["#### AOV plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_aov_cohort1(merchant_name):\n","    selected_tuples = [t for t in main_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 5 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","       group = selected_tuples[group_index][1]\n","       plot_time_series_multiple(\n","            main_df[(main_df['group'] == group) & (main_df['merchant_name'] == merchant_name)], \n","            predicted_main[(predicted_main['group'] == group) & (predicted_main['merchant_name'] == merchant_name)],\n","            'time', aov_variable, \n","            title=f'{merchant_name} [Group {group}\\'s AOV]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","plot_aov_cohort1(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["#### Cohort 1's Sales plot"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698252642807,"user":{"displayName":"Jongseok Han","userId":"03550675782694543507"},"user_tz":420},"id":"hobQYed4GPix"},"outputs":[],"source":["actual_main_sales = actual_main[actual_main['time']>=hyperparam.TEST_START][['merchant_name','time','sales']].groupby(['merchant_name','time']).sum().reset_index()\n","predicted_main_sales = predicted_main[predicted_main['time']>=hyperparam.TEST_START][['merchant_name','time','sales']].groupby(['merchant_name','time']).sum().reset_index()\n","\n","def plot_sales_cohort1(merchant_name):\n","    actual_main_sales0 = actual_main_sales[actual_main_sales['merchant_name']==merchant_name]\n","    predicted_main_sales0 = predicted_main_sales[predicted_main_sales['merchant_name']==merchant_name]\n","    plot_time_series(actual_main_sales0, predicted_main_sales0, 'time', 'sales',\n","        title=f'{merchant_name} [Cohort 1\\'s Sales (Uncensored cohorts)]', formatter=True, ylabel='Sales')\n","\n","plot_sales_cohort1(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Cohort 0 (acquired before the beginning of train period)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sets the module in evaluation mode\n","model.eval()\n","\n","# rolling forecast (scaled)\n","censored_value_test_np = value_dict_to_np(censored_test_dict['scaled_value_seq_dict'], TARGET_TASKS)\n","censored_test_datasets = prepare_TimeSeriesDataset(censored_value_test_np, censored_test_dict['cov_seq'], CrossSectionalTimeSeriesDataset, hyperparam.INPUT_CHUNK_LENGTH)\n","censored_value_test_pred = rolling_forecast(censored_test_datasets, model, device, TARGET_TASKS, verbose=True)\n","\n","# get ground truth in the test period\n","censored_df_test_net = censored_df_test[censored_df_test['time']>=hyperparam.TEST_START][censored_df_test['tenure']>=0]\n","actual_censored = censored_df_test_net[['merchant_name','group', 'time', 'cohort_size'] + TARGET_TASKS]\n","\n","# scale back the forecast\n","predicted_censored = util.inverse_scale_np_to_dataframe_embedding(censored_test_dict['scaler'], censored_test_dict['group_seq'], censored_test_dict['time_seq'], \n","                                                                  censored_value_test_pred, hyperparam.INPUT_CHUNK_LENGTH)\n","predicted_censored.sort_values(['merchant_name', 'group', 'time'], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### Cohort 0's sales recovery"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if PREDICTION_GOAL == 'mtl_4tasks':\n","    acq_variable, ropc_variable, aov_variable, _ = TARGET_TASKS\n","elif PREDICTION_GOAL == 'mtl_3tasks':\n","    acq_variable, ropc_variable, aov_variable = TARGET_TASKS\n","else:\n","    raise ValueError('PREDICTION_GOAL is not tracked.')\n","\n","## get the average of pseudo acquisition\n","actual_censored_acq = actual_censored[['merchant_name','time', acq_variable]].drop_duplicates().reset_index(drop=True)\n","predicted_censored_acq = predicted_censored[['merchant_name','time', acq_variable]].groupby(['merchant_name','time']).mean().reset_index()\n","\n","actual_censored, predicted_censored = get_sales_recovered_data(predicted_censored_acq, actual_censored, predicted_censored, hyperparam.TEST_START,\n","                                                       USE_EMBEDDING, MERCHANT_NAMES_EMB_INT)"]},{"cell_type":"markdown","metadata":{},"source":["#### Acquisition plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.plots import plot_time_series, plot_time_series_multiple\n","def plot_acq_cohort0(merchant_name):\n","    plot_time_series(actual_censored_acq[actual_censored_acq['merchant_name']==merchant_name], \n","                     predicted_censored_acq[predicted_censored_acq['merchant_name']==merchant_name],\n","                     'time', acq_variable,\n","        title=f'{merchant_name} [Uncensored Cohort 0\\'s Acquisition]',\n","        )\n","\n","plot_acq_cohort0(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["#### Repeat order per customer plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_ropc_cohort0(merchant_name):\n","    selected_tuples = [t for t in censored_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 5 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","       group = selected_tuples[group_index][1]\n","       plot_time_series_multiple(\n","            censored_df[(censored_df['group'] == group) & (censored_df['merchant_name'] == merchant_name)], \n","            predicted_censored[(predicted_censored['group'] == group) & (predicted_censored['merchant_name'] == merchant_name)],\n","            'time', ropc_variable, \n","            title=f'{merchant_name} [Group {group}\\'s Repeat Order per Customer]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","plot_ropc_cohort0(list(MERCHANT_NAMES_EMB_INT.keys())[0])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### AOV plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_aov_cohort0(merchant_name):\n","    selected_tuples = [t for t in censored_test_dict['group_seq'] if t[0] == merchant_name]\n","    fig, axs = plt.subplots(3, 1, figsize=(20, 20))  # 5 rows, 1 column\n","    selected_group_indices = random.sample(range(len(selected_tuples)), 3)\n","    for i, group_index in enumerate(selected_group_indices):\n","       group = selected_tuples[group_index][1]\n","       plot_time_series_multiple(\n","            censored_df[(censored_df['group'] == group) & (censored_df['merchant_name'] == merchant_name)], \n","            predicted_censored[(predicted_censored['group'] == group) & (predicted_censored['merchant_name'] == merchant_name)],\n","            'time', aov_variable, \n","            title=f'{merchant_name} [Group {group}\\'s AOV]',\n","            ax=axs[i]\n","        )\n","    plt.tight_layout()\n","\n","plot_aov_cohort0(list(MERCHANT_NAMES_EMB_INT.keys())[0])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Cohort 0's Sales plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["actual_censored_sales = actual_censored[actual_censored['time']>=hyperparam.TEST_START][['merchant_name','time','sales']].groupby(['merchant_name','time']).sum().reset_index()\n","predicted_censored_sales = predicted_censored[predicted_censored['time']>=hyperparam.TEST_START][['merchant_name','time','sales']].groupby(['merchant_name','time']).sum().reset_index()\n","\n","def plot_sales_cohort0(merchant_name):\n","    actual_censored_sales0 = actual_censored_sales[actual_censored_sales['merchant_name']==merchant_name]\n","    predicted_censored_sales0 = predicted_censored_sales[predicted_censored_sales['merchant_name']==merchant_name]\n","    plot_time_series(actual_censored_sales0, predicted_censored_sales0, 'time', 'sales',\n","        title=f'{merchant_name} [Cohort 0\\'s Sales (Uncensored cohorts)]', formatter=True, ylabel='Sales')\n","\n","plot_sales_cohort0(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Total Sales"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["actual_total_sales = pd.merge(actual_main_sales, actual_censored_sales, on=['merchant_name','time'], how='outer')\n","actual_total_sales['total_sales'] = actual_total_sales['sales_x'] + actual_total_sales['sales_y']\n","\n","pred_total_sales = pd.merge(predicted_main_sales, predicted_censored_sales, on=['merchant_name','time'], how='outer')\n","pred_total_sales['total_sales'] = pred_total_sales['sales_x'] + pred_total_sales['sales_y']\n","\n","def plot_total_sales(merchant_name):\n","    plot_time_series(actual_total_sales[actual_total_sales['merchant_name']==merchant_name], \n","                     pred_total_sales[pred_total_sales['merchant_name']==merchant_name], 'time', 'total_sales',\n","        title=f'{merchant_name} [Total Sales] over test period', formatter=True, ylabel='Sales')\n","    \n","plot_total_sales(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["actual_whole = df[['merchant_name','time','spend']].groupby(['merchant_name','time']).sum().reset_index()\n","actual_whole['time'] = pd.to_datetime(actual_whole['time'], format='%Y-%m-%d')\n","actual_whole['total_sales'] = actual_whole['spend']\n","\n","def plot_total_sales_whole(merchant_name):    \n","    plot_time_series(actual_whole[actual_whole['merchant_name']==merchant_name], \n","                     pred_total_sales[pred_total_sales['merchant_name']==merchant_name], \n","                     'time', 'total_sales',\n","        title=f'{merchant_name} [Total Sales] over entire period', formatter=True, ylabel='Sales')\n","plot_total_sales_whole(list(MERCHANT_NAMES_EMB_INT.keys())[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.plots import save_plots_to_pdf\n","\n","if SAVE_MODE:\n","    # Saving plots to PDF\n","    # Saving traininig and validation losses\n","    save_plots_to_pdf([\n","        (plot_losses, {}),\n","        ], f'{SAVE_EPOCH}/train_loss.pdf')\n","    \n","    # Saving csv files\n","    pd.concat([predicted_censored, predicted_main]).to_csv(f'{SAVE_PREDICT}/all_pred.csv', index=False)\n","    pd.concat([actual_censored, actual_main]).to_csv(f'{SAVE_ACTUAL}/all_actual.csv', index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPxURsHtawxcGy64/iEvkgk","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
